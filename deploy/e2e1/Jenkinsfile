pipeline {
    agent any

    environment {
        NODEJS_HOME = tool 'NodeJS'
        PATH = "${NODEJS_HOME}/bin:${env.PATH}"
        REPORT_DIR = "${WORKSPACE}/reports"
    }

    stages {
        stage("Install Node and Dependencies") {
            steps {
                script {
                    sh 'pnpm install'
                }
            }
        }

        stage("Prepare Test Batches") {
            steps {
                script {
                    echo "Collecting all test files..."
                    def testFiles = sh(
                        script: "find ${WORKSPACE}/test -name '*.e2e-spec.ts'",
                        returnStdout: true
                    ).trim().split("\n").findAll { it }

                    if (testFiles.isEmpty()) {
                        error "âŒ No E2E test files found!"
                    }

                    int batchSize = 3
                    int batchCount = (testFiles.size() / batchSize) + ((testFiles.size() % batchSize) > 0 ? 1 : 0)

                    echo "Total Test Files: ${testFiles.size()}, Batches: ${batchCount}, Batch Size: ${batchSize}"

                    env.TEST_FILES_CSV = testFiles.join(',')
                    env.BATCH_SIZE = batchSize.toString()
                }
            }
        }

        stage("Run Test Batches") {
            steps {
                script {
                    sh "mkdir -p ${REPORT_DIR}"  // Ensure report directory exists
                    def testFiles = env.TEST_FILES_CSV.split(',')
                    int batchSize = env.BATCH_SIZE.toInteger()
                    def parallelStages = [:]

                    int numBatches = (testFiles.size() / batchSize) + ((testFiles.size() % batchSize) > 0 ? 1 : 0)

                    echo "Running ${numBatches} batches of tests..."

                    for (int i = 0; i < numBatches; i++) {
                        int start = i * batchSize
                        int end = Math.min((i + 1) * batchSize, testFiles.size())
                        def batchFiles = testFiles[start..<end].join(' ')
                        def batchReportDir = "${REPORT_DIR}/batch-${i + 1}"  // Unique report directory
                        def batchJsonReport = "${batchReportDir}/jest-results.json"

                        parallelStages["Batch-${i + 1}"] = {
                            script {
                                echo "Running Batch ${i + 1} with files: ${batchFiles}"

                                sh """
                                    mkdir -p '${batchReportDir}'
                                    export JEST_REPORT_DIR='${batchReportDir}'
                                    pnpm test:e2e --testPathPattern='${batchFiles.replaceAll(' ', '|')}' --json --outputFile='${batchJsonReport}'
                                """
                            }
                        }
                    }

                    parallel parallelStages
                }
            }
        }
    }


    post {
    always {
        script {
            sh "ls -lhR ${REPORT_DIR} || echo 'âŒ No reports found!'" // Debugging step

            // Archive artifacts
            archiveArtifacts artifacts: 'reports/**', fingerprint: true

            // Jenkins Base URL
            def jenkinsBaseUrl = env.BUILD_URL // Gets the current build URL

            // Initialize test counters
            int totalPassed = 0
            int totalFailed = 0
            int totalPending = 0
            int totalTestSuites = 0
            int totalPassedTestSuites = 0
            int totalFailedTestSuites = 0
            int totalRuntimeErrors = 0
            int totalTestFiles = 0 // Count of test files executed

            // Get batch count from environment
            def numBatches = env.BATCH_SIZE.toInteger()
            for (int i = 1; i <= numBatches; i++) {
                def reportPath = "reports/batch-${i}/jest_report.html"
                
                // Publish HTML report
                publishHTML([
                    reportName: "E2E Test Report - Batch ${i}",
                    reportDir: "reports/batch-${i}",
                    reportFiles: 'jest_report.html',
                    alwaysLinkToLastBuild: true,
                    keepAll: true
                ])

                // Print report URL in console
                echo "ðŸ”— Report for Batch ${i}: ${jenkinsBaseUrl}artifact/${reportPath}"

                // JSON report path
                def batchJsonReport = "${REPORT_DIR}/batch-${i}/jest-results.json"

                if (fileExists(batchJsonReport)) {
                    echo "ðŸ“‚ Processing report: ${batchJsonReport}"

                    def jsonContent = readJSON text: readFile(batchJsonReport)

                    // Accumulate test metrics
                    totalPassed += jsonContent.numPassedTests
                    totalFailed += jsonContent.numFailedTests
                    totalPending += jsonContent.numPendingTests
                    totalTestSuites += jsonContent.numTotalTestSuites
                    totalPassedTestSuites += jsonContent.numPassedTestSuites
                    totalFailedTestSuites += jsonContent.numFailedTestSuites
                    totalRuntimeErrors += jsonContent.numRuntimeErrorTestSuites
                    totalTestFiles += jsonContent.testResults.size()
                } else {
                    echo "âš ï¸ Warning: Report file not found for Batch ${i}"
                }
            }

            // Print summary results
            echo "ðŸ“Š **E2E Test Summary**"
            echo "âœ… Total Passed Tests: ${totalPassed}"
            echo "âŒ Total Failed Tests: ${totalFailed}"
            echo "â³ Total Pending Tests: ${totalPending}"
            echo "ðŸ—‚ï¸ Total Test Suites: ${totalTestSuites}"
            echo "ðŸŸ¢ Passed Test Suites: ${totalPassedTestSuites}"
            echo "ðŸ”´ Failed Test Suites: ${totalFailedTestSuites}"
            echo "ðŸ’¥ Runtime Errors: ${totalRuntimeErrors}"
            echo "ðŸ“‚ Total Test Files Executed: ${totalTestFiles}"
        }
    }

    success {
        echo "âœ… E2E Tests completed successfully!"
    }

    unstable {
        echo "âš ï¸ E2E Tests encountered issues but completed."
    }

    failure {
        echo "âŒ E2E Tests failed! Check logs."
    }
}
}
